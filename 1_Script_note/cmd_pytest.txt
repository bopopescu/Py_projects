#@@ 10/4/19
#@@ 11/22/19
#@@ 12/13/19

 pytest
# skip test: -m, -v, -k

# @pytest.mark.skip("reason to skip")
# @pytest.mark.skipif(a > 10, reason="Give a reason")

# run selected testes @mark
# pytest -k 

# group tests
# @pytest.mark.Group_name
#pytest -m Group_name

# run except test
# pytest -m "not Group_name"
 
# execute setup b4 or after each test.
# @pytest.fixture(scope="module")
# def setup()
#     global driver
#     driver = webdriver.Chrome()
# Everything after yield executed after test is done 
    #   yield
    #   driver.close()

# def test_reg(setup):
#     pass



##
## pytest command
pytest test_name.py
pytest -v
# Running one test item
(env_3.7) C:\Users\jsun\Documents\Py_projects\Pytest_proj\pytest_02\pytest_proj_1>pytest -v tasks\test_four.py::test_asdict
pytest -v tasks\test_four.py::test_asdict
========================================================================================== test session starts ===========================================================================================
platform win32 -- Python 3.7.2, pytest-5.1.2, py-1.8.0, pluggy-0.13.0 -- c:\users\jsun\documents\py_projects\pytest_proj\pytest_02\env_3.7\scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\jsun\Documents\Py_projects\Pytest_proj\pytest_02\pytest_proj_1
collected 1 item

tasks/test_four.py::test_asdict PASSED

##
(env_3.7) C:\Users\jsun\Documents\Py_projects\Pytest_proj\pytest_02\pytest_proj_1>pytest -v tasks\test_four.py::test_asdict  tasks\test_four.py::test_replace
pytest -v tasks\test_four.py::test_asdict  tasks\test_four.py::test_replace
========================================================================================== test session starts ===========================================================================================
platform win32 -- Python 3.7.2, pytest-5.1.2, py-1.8.0, pluggy-0.13.0 -- c:\users\jsun\documents\py_projects\pytest_proj\pytest_02\env_3.7\scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\jsun\Documents\Py_projects\Pytest_proj\pytest_02\pytest_proj_1
collected 2 items

tasks/test_four.py::test_asdict PASSED                                                                                                                                                              [ 50%]
tasks/test_four.py::test_replace PASSED

## Show results tree-like
(env_3.7) C:\Users\jsun\Documents\Py_projects\Pytest_proj\pytest_02\pytest_proj_1>pytest --collect-only
========================================= test session starts =========================================
platform win32 -- Python 3.7.2, pytest-5.1.2, py-1.8.0, pluggy-0.13.0
rootdir: C:\Users\jsun\Documents\Py_projects\Pytest_proj\pytest_02\pytest_proj_1
collected 7 items
<Module one_test.py>
  <Function test_passing>
<Module test_one.py>
  <Function test_passing>
<Module test_two.py>
  <Function test_failing>
<Module tasks/test_four.py>
  <Function test_asdict>
  <Function test_replace>
<Module tasks/test_three.py>
  <Function test_defaults>
  <Function test_member_access>

##
pytest  -k "asdict or defaults" --collect-only

##
(env_3.7) C:\Users\jsun\Documents\Py_projects\Pytest_proj\pytest_02\pytest_proj_1>pytest -v -m run_these_please
pytest -v -m run_these_please
========================================================================================== test session starts ===========================================================================================
platform win32 -- Python 3.7.2, pytest-5.1.2, py-1.8.0, pluggy-0.13.0 -- c:\users\jsun\documents\py_projects\pytest_proj\pytest_02\env_3.7\scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\jsun\Documents\Py_projects\Pytest_proj\pytest_02\pytest_proj_1
collected 7 items / 6 deselected / 1 selected

tasks/test_four.py::test_asdict PASSED

## skip the rest if fail
pytest -x

## turn off stack - trace
pytest -v --tb=no
pytest --maxfail=1 --tb=no

## output print statement
pytest --capture=no
pytest -s

## Run last failed test
pytest --lf

## run the first failed and the rest.
pytest --ff
C:\Program Files\MongoDB\Server\4.2\data\
## pip local install
(env_3.7) C:\Users\jsun\Documents\Py_projects\Pytest_proj\pytest_02\code\tasks_proj>pip install .
Processing c:\users\jsun\documents\py_projects\pytest_proj\pytest_02\code\tasks_proj
Requirement already satisfied: click in c:\users\jsun\documents\py_projects\pytest_proj\pytest_02\env_3.7\lib\site-packages (from tasks==0.1.0) (7.0)
Requirement already satisfied: tinydb in c:\users\jsun\documents\py_projects\pytest_proj\pytest_02\env_3.7\lib\site-packages (from tasks==0.1.0) (3.14.1)
Requirement already satisfied: six in c:\users\jsun\documents\py_projects\pytest_proj\pytest_02\env_3.7\lib\site-packages (from tasks==0.1.0) (1.12.0)
Installing collected packages: tasks
  Running setup.py install for tasks ... done
Successfully installed tasks-0.1.0
## Run a selective tests using -k expression
pytest -v -k "_raises and not delete"


##
pytest -v test_add.py -k valid_id
## Run to show fixture process
pytest --setup-show test_add.py -k valid_
##
(env_3.7) C:\Users\jsun\Documents\Py_projects\Pytest_proj\pytest_02\pytest_proj_3\tasks_proj\tests\func>pytest -v --setup-show test_add.py -k valid_id
================================================================================ test session starts ================================================================================
platform win32 -- Python 3.7.2, pytest-5.1.2, py-1.8.0, pluggy-0.13.0 -- c:\users\jsun\documents\py_projects\pytest_proj\pytest_02\env_3.7\scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\jsun\Documents\Py_projects\Pytest_proj\pytest_02\pytest_proj_3\tasks_proj\tests, inifile: pytest.ini
collected 3 items / 2 deselected / 1 selected

test_add.py::test_add_returns_valid_id
SETUP    S tmp_path_factory
        SETUP    F tmp_path (fixtures used: tmp_path_factory)
        SETUP    F tmpdir (fixtures used: tmp_path)
        SETUP    F tasks_db (fixtures used: tmpdir)
        func/test_add.py::test_add_returns_valid_id (fixtures used: tasks_db, tmp_path, tmp_path_factory, tmpdir)PASSED
        TEARDOWN F tasks_db
        TEARDOWN F tmpdir
        TEARDOWN F tmp_path
TEARDOWN S tmp_path_factory

####
INSTALL FROM A LOCAL DIRECTORY
You can keep a local stash of plugins (and other Python packages) in a local or shared directory in .tar.gz or .whl format 
and use that instead of PyPI for installing plugins:

mkdir?? ??some_plugins?
cp pytest_cov-2.4.0-py2.py3-none-any.whl some_plugins/
pip install --no-index --find-links=./some_plugins/ pytest-cov
The --no-index tells pip to not connect to PyPI. The --find-links=./some_plugins/ tells pip to look in the directory called some_plugins. 

#### Note that with the local directory install method, you can install multiple versions and specify which version you want by adding == and the version number:
pip install --no-index --find-links=./some_plugins/ pytest-cov==2.4.0

### Install From A Git Repository
pip install git+https://github.com/pytest-dev/pytest-cov@v2.4.0
# specify a branch
pip install git+https://github.com/pytest-dev/pytest-cov@master

### Create your own plugin
###In this section, we�ll develop a small modification to pytest behavior, package it as a plugin, test it, and look into how to distribute it.

###Plugins can include hook functions that alter pytest�s behavior.



##
Pytest syntax for writing tests-
1. File names should start with “test_”, or end with "_test".
2. If tests are defined as methods on a class, the class name should start with “Test".
    The class should not have an __init__ method.
3. Test method names or function names should start with “test_”.

Methods with names that don’t match this pattern won’t be executed as tests.

#@@ 1.
1)
python -m pytest -v

2) Run tests by substring matching of test method
pytest -k method2 -v

3) Run tests by markers @pytest.mark
pytest -m set1

4) parallel test
pytest -n 4 -v

5) @pytest.fixture
Fixtures can be used to share test data between tests, execute setup and teardown methods before
and after test executions respectively.

function defined in fixture will run once before each test function.

import pytest
@pytest.fixture
def get_sum_test_data():
        return [(3,5,8), (-2,-2,-4), (-1,5,4), (3,-5,-2), (0,5,5)]
def test_sum(get_sum_test_data):
        for data in get_sum_test_data:
                num1 = data[0]
                num2 = data[1]
                expected = data[2]
                assert sum(num1, num2) == expected

** Scope of fixture- Scope controls how often a fixture gets called. 
The default is function.
Here are the options for scope:
function: Run once per test
class: Run once per class of tests
module: Run once per module
session: Run once per session
autouse: default is False, if True, all the tests will run the fixture.

scope="session"If we need to perform an action before and after for a set of methods
in a folder or project we session scope (scope=“session”). 
It creates single fixture for set of methods in a project or modules in some path.

5.1) define @pytest.fixture in  conftest.py so multiple test_.py
can use it

6) Parameterized tests
@pytest.mark.parametrize("input1, input2, output",[(5,5,10),(3,5,12)])


@pytest.fixture(scope='session')
def get_sum_test_data():
        return [(3,5,8), (-2,-2,-4), (-1,5,4), (3,-5,-2), (0,5,5)]
@pytest.fixture(autouse=True)
def setup_and_teardown():
        print '\nFetching data from db'
        yield
        print '\nSaving test run data in db'
def test_sum(get_sum_test_data):
        for data in get_sum_test_data:
                num1 = data[0]
                num2 = data[1]
                expected = data[2]
                assert sum(num1, num2) == expected

7) Xfail / Skip tests

execute tests with recording either pass or fail status
@pytest.mark.xfail

completely skip test
@pytest.mark.skip

****
The builtin pytest.mark.parametrize decorator enables parametrization of arguments for a test function.
We have passed following parameters to it-
argnames — a comma-separated string denoting one or more argument names, or a list/tuple of argument strings.
Here, we have passed num1, num2 and expected as 1st input , 2nd input and expected sum respectively.
argvalues — The list of argvalues determines how often a test is invoked with different argument values.
If only one argname was specified argvalues is a list of values. If N argnames were specified,
argvalues must be a list of N-tuples, where each tuple-element specifies a value for its respective argname.
Here, we have passed a tuple of (3,5,8) inside a list where 3 is num1,5 isnum2 and 8is expected sum.

@pytest.mark.parametrize('num1, num2, expected',[(3,5,8), (-2,-2,-4), (-1,5,4), (3,-5,-2), (0,5,5)])
def test_sum(num1, num2, expected):
        assert sum(num1, num2) == expected

In above code, we have passed the values of 2nd argument(which are actual test data) directly there.
We can also make a function call to get those values.
import pytest
def get_sum_test_data():
        return [(3,5,8), (-2,-2,-4), (-1,5,4), (3,-5,-2), (0,5,5)]
@pytest.mark.parametrize('num1, num2, expected',get_sum_test_data())
def test_sum(num1, num2, expected):
        assert sum(num1, num2) == expected
_____________________________________________________
Summary

Install pytest using pip install pytest=2.9.1
Simple pytest program and run it with py.test command.
Assertion statements, assert x==y, will return either True or False.
How pytest identifies test files and methods.
Test files starting with test_ or ending with _test
Test methods starting with test
py.test command will run all the test files in that folder and subfolders. To run a specific file, we can use the command py.test <filename>
Run a subset of test methods
Grouping of test names by substring matching.

py.test -k <name> -v will run all the tests having <name> in its name.

Run test by markers.Mark the tests using @pytest.mark.<name> and run the tests using
pytest -m <name> to run tests marked as <name>.
@pytest.mark.set2
def test_file1_method1():
    x, y = 5, 6
    assert x+1 == y, "x + 1 == y failed"
    #assert x == y, "x==y failed"
    assert x == y, "test failed because x=" + str(x) + " y=" + str(y)
python -m pytest -m set1


Run tests in parallel
Install pytest-xdist using pip install pytest-xdist
Run tests using py.test -n NUM where NUM is the number of workers

Creating fixture methods to run code before every test by marking the method with @pytest.fixture
The scope of a fixture method is within the file it is defined.
A fixture method can be accessed across multiple test files by defining it in conftest.py file.
A test method can access a fixture by using it as an input argument.
Parametrizing tests to run it against multiple set of inputs.

@pytest.mark.parametrize("input1, input2, output",[(5,5,10),(3,5,12)]) def test_add(input1, input2, output):
assert input1+input2 == output,"failed"
will run the test with inputs (5,5,10) and (3,5,12)
Skip/xfail tests using @pytets.mark.skip and @pytest.mark.xfail
Create test results in XML format which covers executed test details using py.test test_sample1.py -v --junitxml="result.xml"
A sample pytest framework to test an API
___________________________________________________________

# keyword expressions
# Run all tests with some string ‘validate’ in the name
pytest -k “validate”
# Exclude tests with ‘db’ in name but include 'validate'
pytest -k “validate and not db”
#Run all test files inside a folder demo_tests
pytest demo_tests/

# Run a single method test_method of a test class TestClassDemo
pytest demo_tests/test_example.py::TestClassDemo::test_method
# Run a single test class named TestClassDemo
pytest demo_tests/test_example.py::TestClassDemo
# Run a single test function named test_sum
pytest demo_tests/test_example.py::test_sum
# Run tests in verbose mode:
pytest -v demo_tests/
# Run tests including print statements:
pytest -s demo_tests/
# Only run tests that failed during the last run
pytest — lf

____________________________________________________________
8) 
Coverage Test
# 
pip install coverage
coverage run test.py arg1 arg2
coverage report -m
coverage html

9) conftest.py

10) mock, mockito

11) examples
@pytest.fixture
def supply_AA_BB_CC():
    aa=25
    bb =35
    cc=45
    return [aa,bb,cc]


python -m pytest -v test_ex1.py -m set1
test_ex1.py::test_file1_method2 PASSED
@pytest.mark.set1
def test_file1_method2():
    x, y = 5, 6
    assert x + 1 == y, "x + 1 == y Failed"


@pytest.mark.parametrize("input1, input2, output",[(5,5,10),(3,5,12)])
def test_add(input1, input2, output):
	assert input1+input2 == output, "failed"

@pytest.mark.skip
def testadd_2():
	assert 100+200 == 300,"failed"


@pytest.mark.xfail
def testadd_3():
	assert 15+13 == 28,"failed"

12)

